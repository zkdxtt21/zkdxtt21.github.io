---
layout:     post
title:      Paper Reading -- Graph Attention Networks
subtitle:   Graph Attention Networks (Veličković et al., ICLR 2018)
date:       2021-03-18
author:     Tingting
header-img: img/post-GAT.png
catalog: true
tags:
    - Paper Reading
    - Graph Neural Network
    - Attention
    - GAT
---

# Basic Information
Graph Attention Networks (Veličković et al., ICLR 2018): https://arxiv.org/abs/1710.10903
## Git Repo
https://github.com/PetarV-/GAT
## Reference
If you make advantage of the GAT model in your research, please cite the following in your manuscript:
```
@article{
  velickovic2018graph,
  title="{Graph Attention Networks}",
  author={Veli{\v{c}}kovi{\'{c}}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`{o}}, Pietro and Bengio, Yoshua},
  journal={International Conference on Learning Representations},
  year={2018},
  url={https://openreview.net/forum?id=rJXMpikCZ},
  note={accepted as poster},
}
```

# Reading Notes
## Idea
Attention mechanisms allow for dealing with variable sized inputs, focusing on the most relevant part of the input to make decisions. When an attention mechanism is used to compute a representation of a single sequence, it is commonly referred to as self-attention or intra-attention.

Inspired by the success of attention mechanisms in many sequence-based tasks, they introduced this attention-based architechture to perform node classification of graph-structured data.

The idea is to compute the hidden representations of each node in the graph, by attending over its neighbors, following a self-attention strategy.

## Properties of attention architecture
1. the operation is efficient, since it is parallelizable across node-neighbor pairs
2. it can be applied to graph nodes having different degrees by specifying arbitrary weights to the neighbors.
3. the model is directly applicable to inductive learning problems, including tasks where the model has to generalize to completely unseen graphs.

## GAT Architechture
- Graph Attentional Layer
    - perform self-attention on nodes - a shared attentional mechanism a, which computes attention coefficients:  
        ![](https://raw.githubusercontent.com/zkdxtt21/zkdxtt21.github.io/master/_posts/images/GAT-eq1.png)
    - normalize the coefficients using the softmax function to makethem easily comparable across different nodes:  
        ![](https://raw.githubusercontent.com/zkdxtt21/zkdxtt21.github.io/master/_posts/images/GAT-eq2.png)
    - In their experiments, the structure they use:  
        ![](https://raw.githubusercontent.com/zkdxtt21/zkdxtt21.github.io/master/_posts/images/GAT-eq3.png)  
    Here || is the concatenation operation.
    - Once obtained, compute a linear combination using these normalized attention coefficients to sever as the final output features for every node (after potentially applying a nonlinearity, $\sigma$):  
        ![](https://raw.githubusercontent.com/zkdxtt21/zkdxtt21.github.io/master/_posts/images/GAT-eq4.png)
    - multi-head attention:  
        ![](https://raw.githubusercontent.com/zkdxtt21/zkdxtt21.github.io/master/_posts/images/GAT-eq5.png)  
    It implies K independent attention mechanisms execute the tranforamtion of last equation, and then their features are concatenated.
    - Specially, for the final (prediction) layer, we employ averaging, and delay applying the final nonlineariry until then:  
        ![](https://raw.githubusercontent.com/zkdxtt21/zkdxtt21.github.io/master/_posts/images/GAT-eq6.png)

## Pros compared with other methods:
- Computationally, it is highly efficient.   
Time complexity of a single GAT: O(|V|FF' + |E|F'), where F is the number of input features, and |V| and |E| are the numbers of nodes and edges in the graph, respectively.
- As opposed to GCNs, allows for (implicitly) assigning different importances to nodes of a same neighborhodd, enabling a leap in model capacity. Furthermore, leads to a benefits in interpretability.
- Does not depend on upfront access to the global graph structure or (features of) all of its nodes. It also means:  
    - the graph is not required to be undirected
    - directly applicable to inductive learning
- Compared with method of Hamilton et al. (2017), this model works with the entirety of the neighborhood, and does not assume any ordering within it.
- GAT can be reformulated as a particular instance of MoNet (Monti et al., 2016). Difference is our model uses node features for similarity computations, rather than the node’s structural properties.

## Evaluation
###  Validation on chanlleging benchmarks
- Transductive
    - Cora
    - Citesser
    - Pubmed citation networks

    Nodes features correspond to elements of a bag-of-works representation of a document. Each node has a class label. Only 20 nodes per class are used for training. But the training algorithm has access to all of the nodes' featrue vectors.
- Inductive
    - Protein-protein interaction dataset (PPI)

    Nodes featuresare composed of positional gene sets, motif gene sets and immunological signatures. There are 121 labels for each node set from gene ontology, collected from the Molecular Signatures Database, and a node can poseess several lables simutaneously.

![](https://raw.githubusercontent.com/zkdxtt21/zkdxtt21.github.io/master/_posts/images/GAT-dataset.png)

### Transductive Learning
Follow the experimental setup in Yang et al. (2016). Training, validation and testing settings are the same as Kipf & Wellin (2017). 

They compare GAT against the same strong baselines and state-of-the-art approaches as specified in Kipf & Wellin (2017).

- Settings of GAT (two-layer)
    - Cora and Citeseer: 
        - first layer: # of attention head: K=8, F'=8 features each, 64 in total.  
        nonlinearity function: ELU
        - second layer: # of attention head: K=1, F'=C, C is the number of classes.  
        nonlinearity function: Softmax
        - L2 regularization with $\lambda=0.0005$, dropout with $p=0.6$.
        - Glorot initialization
        - Adam SGD optimizer
        - learning rate: 0.005
    - Pubmed:
        - second layer: K=8
        - L2 regularization with $\lambda=0.001$
        - all the other are the same as previous one
        - learning rate: 0.01

- Metric for results: mean classification accuracy (with standard deviation)

![](https://raw.githubusercontent.com/zkdxtt21/zkdxtt21.github.io/master/_posts/images/GAT-result1.png)

### Inductive Learning
Use preprocessed PPI data provided by Hamilton et al. (2017). 

They compare against the four different supervised Graph SAGE inductive methods presented in Hamilton et al. (2017). To strictly evaluate the benefits of applying the attention mechanism, they also compare the results using the same architecture with a constant attention mechanism.

- Settings of GAT (three-layer)
    - first two layers: K=4, F'=256, nonlinearity function: ELU
    - third layer: K=6, F'=121, nonlinearity function: logistic sigmoid activation
    - no L2 regularization and no dropout
    - learning rate: 0.005

- Metric for results: micro-averaged F1 score on the nodes of the two unseen test graphs

![](https://raw.githubusercontent.com/zkdxtt21/zkdxtt21.github.io/master/_posts/images/GAT-result2.png)

### Effectiveness of the learned feature representations using tsne
![](https://raw.githubusercontent.com/zkdxtt21/zkdxtt21.github.io/master/_posts/images/GAT-tsne.png)

# Reference Papers Interested
- William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. Neural Information Processing Systems (NIPS), 2017. (https://arxiv.org/abs/1706.02216)

- Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. International Conference on Learning Representations (ICLR), 2015. (https://arxiv.org/pdf/1409.0473.pdf)

- Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with
graph embeddings. In International Conference on Machine Learning, pp. 40–48, 2016. (http://proceedings.mlr.press/v48/yanga16.pdf)

- Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. International Conference on Learning Representations (ICLR), 2017. (https://arxiv.org/pdf/1609.02907.pdf?source=post_page)
